name: Scrape & Parse Events

on:
  workflow_dispatch: {}
  schedule:
    - cron: "7 */3 * * *"   # every 3 hours @ :07

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # playwright is in requirements.txt, this installs browser binaries:
          python -m playwright install --with-deps chromium

      - name: Run scraper
        env:
          USE_PLAYWRIGHT: "1"
        run: |
          set -euo pipefail
          mkdir -p state/snapshots
          # IMPORTANT: feed sources.yml on stdin (your main.py expects it)
          python src/main.py < sources.yml

          # Sanity check: fail fast if the scraper made nothing
          test -s state/last_run_report.json || (echo "last_run_report.json not created" && exit 1)

      - name: Upload scrape artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scrape-output
          retention-days: 7
          path: |
            state/last_run_report.json
            state/snapshots/**
            northwoods*.ics
